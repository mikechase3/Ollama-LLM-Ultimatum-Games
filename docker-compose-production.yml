services:
  # Service #1: The Ollama LLM Server
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ./ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    # PRODUCTION CONFIGURATION: Pulls ALL models listed in input_table_gen.py
    # Warning: This is roughly 100GB+ of downloads.
    command: >
      sh -c "
        ollama serve & 
        sleep 5 && 
        ollama pull phi3:latest &&
        ollama pull dolphin-llama3:8b &&
        ollama pull mixtral:8x7b &&
        ollama pull llama2:70b &&
        ollama pull falcon:40b &&
        ollama pull openchat:7b &&
        ollama pull vicuna:13b &&
        ollama pull codellama:34b &&
        ollama pull qwen:72b &&
        wait
      "

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - ./webui-data:/app/backend/data
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  # Service 3: Python Experiment Runner
  python-dev:
    build: .
    container_name: python-dev
    volumes:
      - ./:/app
    ports:
      - "8501:8501"
      - "2222:22"
    command: sh -c "service ssh start && streamlit run src/dashboard.py"
    depends_on:
      - ollama
    restart: unless-stopped