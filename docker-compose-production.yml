services:
  # Service #1: The Ollama LLM Server
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ./ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped

    # --- GPU CONFIGURATION START ---
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # --- GPU CONFIGURATION END ---

    # Use this exact structure for the entrypoint and command:
    entrypoint: ["/bin/sh", "-c"]

    command: >
      "ollama serve & 
      sleep 5 && 
      ollama pull phi3:latest &&
      ollama pull dolphin-llama3:8b &&
      ollama pull openchat:7b &&
      ollama pull mistral:7b &&
      ollama pull gemma2:9b &&
      ollama pull hermes3:8b &&
      wait"

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - ./webui-data:/app/backend/data
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  # Service 3: Streamlit Dashboard
  dashboard:
    build: .
    container_name: dashboard
    volumes:
      - ./:/app
    ports:
      - "8501:8501"
      - "2222:22"
    command: sh -c "service ssh start && streamlit run src/dashboard.py"
    depends_on:
      - ollama
    restart: unless-stopped