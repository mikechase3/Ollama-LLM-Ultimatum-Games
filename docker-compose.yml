services:
  # Service #1: The Ollama LLM Server
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      # Maps local folder `ollama-data` to the container's model storage. All models saved here.
      - ./ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - ./webui-data/app/backend/data
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  # Service 3: my python app & experiment runner & streamlit thing
  python-dev:
    build: .
    volumes:
      - ./:/app
    ports:
      - "8501:8501"
    command: streamlit run src/app.py --server .runOnSave=true
    depends_on:
      - ollama
    restart: unless-stopped
