services:
  # Service #1: The Ollama LLM Server
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      # Maps local folder `ollama-data` to the container's model storage. All models saved here.
      - ./ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - ./webui-data:/app/backend/data
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

  # Service 3: my python app & experiment runner & streamlit thing
  python-dev:
    build: .
    container_name: python-dev
    volumes:
      - ./:/app
    ports:
      - "8501:8501"  # Map Streamlit App
      - "2222:22"  # Map host port 2222 to container's SSH port on 22
    command: sh -c "service ssh start && streamlit run src/dashboard.py"
#    command: streamlit run src/dashboard.py --server.runOnSave=true
#    command: sleep infinity
    depends_on:
      - ollama
    restart: unless-stopped
